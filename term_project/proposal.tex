%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% fphw Assignment
% LaTeX Template
% Version 1.0 (27/04/2019)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Authors:
% Class by Felipe Portales-Oliva (f.portales.oliva@gmail.com) with template 
% content and modifications by Vel (vel@LaTeXTemplates.com)
%
% Template (this file) License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	12pt, % Default font size, values between 10pt-12pt are allowed
	%letterpaper, % Uncomment for US letter paper size
	%spanish, % Uncomment for Spanish
]{../Template/fphw}

% Template-specific packages
\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{mathpazo} % Use the Palatino font

\usepackage{graphicx} % Required for including images

\usepackage{booktabs} % Required for better horizontal rules in tables

\usepackage{listings} % Required for insertion of code

\usepackage{enumerate} % To modify the enumerate environment

% Additional packages needed
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{mwe}
\usepackage{comment}

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Proposal: Transformers and Its Sensitivity} % Assignment title

\author{Mao Nishino, Farez Siddiqui} % Student name

\date{April 11th, 2024} % Due date

\institute{Florida State University \\ Department of Computer Science} % Institute or school name

\class{Deep and Reinforcement Learning Fundamentals (CAP5619-0001.sp24)} % Course or class name

\professor{Dr. Xiuwen Liu} % Professor or teacher in charge of the assignment

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Output the assignment title, created automatically using the information in the custom commands above

\section*{Project Issue and Goals} 
In the past 8 years since Transformers \cite{att} was born, the field of deep learning has progressed dramatically, and many state-of-the-art models rely on Transformers nowadays. The emergence of GPT models \cite{radford2018improving} changed not only the academic deep-learning community but also society as a whole by introducing a highly intelligent agent to everyday life. On the other hand, although it is already used extensively in the workforce, its safety is still an active area of research. In particular, in the field of computer vision, the models based on Vision Transformers (ViT) are known to be very sensitive to adversarial examples, which are examples made to deceive recognition models \cite{salman2024intriguing}. To understand this phenomenon better, we believe that a deeper understanding of the dynamics of Transformers by using tools from theoretical and computational mathematics, is necessary. \\

This project will focus on the sensitivity of Transformers with respect to the input data. In particular, our goal is to \textbf{theoretically and numerically verify the sensitive nature of Vision Transformers based on principles from analysis and geometry}. Our aims are described as follows:

\begin{enumerate}
    \item \textbf{Theoretical and numerical investigation of sensitivity for models proposed by \cite{geshkovski2024mathematical}}. Geshkovski et al. viewed Transformers as a partial differential equation on a high-dimensional sphere. Moreover, its connection to gradient flows, which generalizes the notion of gradient descent to the space of probability distributions, was drawn. Using the tools from optimal transport and dynamical systems, we will seek a qualitative and quantitative understanding of the differential equation modeling Transformers.
    \item \textbf{Reconstruction and generalization of several empirical results regarding Vision Transformers.} Although ViT-based models are not completely immune to adversarial examples, it is understood that ViT-based models show better robustness compared to convolutional networks, which are used commonly for image recognition tasks. Following \cite{naseer2021intriguing}, \cite{zhou2022understanding}, and \cite{salman2024intriguing}, we reconstruct their results and see if they generalize to more recent models. 

    
\end{enumerate}

\bibliographystyle{plain}
\bibliography{term_project/bib_term_project.bib}

\end{document}
