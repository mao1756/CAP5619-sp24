{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b57370-fa85-4cf1-9bac-6ef8b18a1ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('Device name:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bb66920-ef4c-49ea-b0e7-b51a29de7a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/farez/amazon_reviews.csv')\n",
    "del df[df.columns[0]] # first column is just index so we drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc939dbb-e0de-48f3-a882-c87b870be1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "text = df['reviewText'].tolist()\n",
    "text = [item for item in text if isinstance(item, str)] # removing any reviews that are not strings\n",
    "dataset = MyDataset(text)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_text = [train_dataset[i] for i in range(len(train_dataset))]\n",
    "test_text = [test_dataset[i] for i in range(len(test_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c1175-482a-4bea-91dd-5eb3465ce127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/492 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use the end of sequence token as padding token.\n",
    "encoded_inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "embeddings = []\n",
    "# obtain embeddings\n",
    "\n",
    "chunk = 10\n",
    "for i in tqdm(range(0,len(encoded_inputs['input_ids']),chunk)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_inputs['input_ids'][i:i+chunk])\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        embeddings.append(torch.mean(hidden_states, dim=1))  # mean pooling\n",
    "\n",
    "# print(embeddings)\n",
    "embeddings_cat = torch.cat(embeddings, dim=0)\n",
    "torch.save(embeddings_cat, 'embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294e979f-b10d-4243-8188-e758da4794a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.load(\"embeddings.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07ff9896-4f40-4e82-a50e-29b7f8dc2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class GPT2ForClassification(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(GPT2ForClassification, self).__init__()\n",
    "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.gpt2.config.n_embd, num_labels)\n",
    "\n",
    "    def forward(self, inputs_embeds, attention_mask=None):\n",
    "        outputs = self.gpt2(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state[:, -1, :]  # Use the last token's output for classification\n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        logits = self.classifier(hidden_state)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1038ae0-76d4-40dd-87f6-9bca8eb112cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a39ccc3-6703-42ff-a617-4b6a5902bf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of non-string values in column 'reviewText': [125]\n",
      "4914\n"
     ]
    }
   ],
   "source": [
    "# need to remove the same index from the ratings that we removed earlier from the reviews that was \"not-a-string\"\n",
    "non_string_review = df['reviewText'].apply(lambda x: not isinstance(x, str))\n",
    "non_string_indices = non_string_review[non_string_review].index\n",
    "print(\"Indices of non-string values in column 'reviewText':\", non_string_indices.tolist())\n",
    "\n",
    "ratings = df['overall'].drop(df['overall'].index[non_string_indices.values[0]]) # removal of problem index from the ratings\n",
    "print(len(ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06ef1cba-6787-48b1-aa49-d2eabf221616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert tensors to numpy arrays for splitting\n",
    "embeddings_np = embeddings.numpy()\n",
    "ratings = ratings.to_numpy().astype(int) # convert to integer numpy array for classification\n",
    "\n",
    "# Split the data\n",
    "embeddings_train, embeddings_test, labels_train, labels_test = train_test_split(\n",
    "    embeddings_np, ratings, test_size=0.20)\n",
    "\n",
    "# Convert numpy arrays back to tensors\n",
    "embeddings_train = torch.tensor(embeddings_train)\n",
    "embeddings_test = torch.tensor(embeddings_test)\n",
    "labels_train = torch.tensor(labels_train)\n",
    "labels_test = torch.tensor(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bce1f393-25b3-4180-89c0-00635f22d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/246 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "GPT2ForClassification.forward() missing 1 required positional argument: 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embeddings_batch, labels_batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(embeddings_batch)\n\u001b[0;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels_batch)\n\u001b[0;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: GPT2ForClassification.forward() missing 1 required positional argument: 'attention_mask'"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create dataset and dataloader for more efficient training\n",
    "train_dataset = TensorDataset(embeddings_train, labels_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = GPT2ForClassification(num_labels=5)\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # number of epochs\n",
    "    for embeddings_batch, labels_batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(embeddings_batch)\n",
    "        loss = loss_fn(logits, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e266d968-ea08-4d00-a33a-982658a7695d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4915"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits_test = model(embeddings_test)\n",
    "    predictions = torch.argmax(logits_test, dim=1)\n",
    "    accuracy = accuracy_score(labels_test, predictions.numpy())\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55a1b8f9-90d8-40c7-af0e-f557dbbb084c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      reviewerName  overall  \\\n",
      "0              NaN      4.0   \n",
      "1             0mie      5.0   \n",
      "2              1K3      4.0   \n",
      "3              1m2      5.0   \n",
      "4     2&amp;1/2Men      5.0   \n",
      "...            ...      ...   \n",
      "4910        ZM \"J\"      1.0   \n",
      "4911            Zo      5.0   \n",
      "4912     Z S Liske      5.0   \n",
      "4913      Z Taylor      5.0   \n",
      "4914           Zza      5.0   \n",
      "\n",
      "                                             reviewText  reviewTime  day_diff  \\\n",
      "0                                            No issues.  2014-07-23       138   \n",
      "1     Purchased this for my device, it worked as adv...  2013-10-25       409   \n",
      "2     it works as expected. I should have sprung for...  2012-12-23       715   \n",
      "3     This think has worked out great.Had a diff. br...  2013-11-21       382   \n",
      "4     Bought it with Retail Packaging, arrived legit...  2013-07-13       513   \n",
      "...                                                 ...         ...       ...   \n",
      "4910  I bought this Sandisk 16GB Class 10 to use wit...  2013-07-23       503   \n",
      "4911  Used this for extending the capabilities of my...  2013-08-22       473   \n",
      "4912  Great card that is very fast and reliable. It ...  2014-03-31       252   \n",
      "4913  Good amount of space for the stuff I want to d...  2013-09-16       448   \n",
      "4914  I've heard bad things about this 64gb Micro SD...  2014-02-01       310   \n",
      "\n",
      "      helpful_yes  helpful_no  total_vote  score_pos_neg_diff  \\\n",
      "0               0           0           0                   0   \n",
      "1               0           0           0                   0   \n",
      "2               0           0           0                   0   \n",
      "3               0           0           0                   0   \n",
      "4               0           0           0                   0   \n",
      "...           ...         ...         ...                 ...   \n",
      "4910            0           0           0                   0   \n",
      "4911            0           0           0                   0   \n",
      "4912            0           0           0                   0   \n",
      "4913            0           0           0                   0   \n",
      "4914            0           0           0                   0   \n",
      "\n",
      "      score_average_rating  wilson_lower_bound  \n",
      "0                      0.0                 0.0  \n",
      "1                      0.0                 0.0  \n",
      "2                      0.0                 0.0  \n",
      "3                      0.0                 0.0  \n",
      "4                      0.0                 0.0  \n",
      "...                    ...                 ...  \n",
      "4910                   0.0                 0.0  \n",
      "4911                   0.0                 0.0  \n",
      "4912                   0.0                 0.0  \n",
      "4913                   0.0                 0.0  \n",
      "4914                   0.0                 0.0  \n",
      "\n",
      "[4915 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c876ce7-36dd-4337-bda0-97574a58c2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
