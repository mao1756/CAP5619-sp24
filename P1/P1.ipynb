{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Mao Nishino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Loaded 7291 data for train data\n",
      "Loaded 2007 data for train data\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "def load_data(path: str):\n",
    "    \"\"\" Load the data and make a list of (index, data tensor).\n",
    "    Args:   \n",
    "        path: the path of the file to read  the data from.\n",
    "    Returns:\n",
    "        data_list: a list of tuples, each tuple contains the index of the data and the data tensor.\n",
    "    \"\"\"\n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    float_lines = [[float(string) for string in line.split()] for line in lines]\n",
    "    data_list = [(torch.tensor(line[1:]).reshape(16,16), int(line[0])) for line in float_lines]\n",
    "    return data_list\n",
    "\n",
    "train_list = load_data('zip_train.txt')\n",
    "test_list = load_data('zip_test.txt')\n",
    "\n",
    "print(f\"Loaded {len(train_list)} data for train data\")\n",
    "print(f\"Loaded {len(test_list)} data for train data\")\n",
    "\n",
    "# Define PyTorch Dataset\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "\n",
    "# Create PyTorch Dataset\n",
    "train_dataset = MyDataset(train_list)\n",
    "test_dataset = MyDataset(test_list)\n",
    "\n",
    "# Split the train dataset into train and validation\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (n1, n2, n3): [982, 794, 564], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 53.2%\n",
      "Epochs: 5, Batch size: 32\n",
      "Best (n1, n2, n3): [430, 326, 259], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 61.1%\n",
      "Epochs: 5, Batch size: 32\n",
      "Best (n1, n2, n3): [1010, 945, 786], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 83.8%\n",
      "Epochs: 5, Batch size: 32\n",
      "Best (n1, n2, n3): [1017, 779, 760], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 89.4%\n",
      "Epochs: 5, Batch size: 32\n",
      "Best (n1, n2, n3): [1000, 994, 895], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 90.5%\n",
      "Epochs: 5, Batch size: 32\n",
      "Best (n1, n2, n3): [1014, 922, 374], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 91.0%\n",
      "Epochs: 5, Batch size: 32\n",
      "Best (n1, n2, n3): [1015, 1013, 1007], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 91.2%\n",
      "Epochs: 5, Batch size: 32\n",
      "Now the best n is [1015, 1013, 1007] and the number of samples is 100 and the number of epochs is 15 and the batch size is 16\n",
      "Best (n1, n2, n3): [632, 161, 41], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 95.1%\n",
      "Epochs: 15, Batch size: 16\n",
      "Best (n1, n2, n3): [1000, 979, 936], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 95.3%\n",
      "Epochs: 15, Batch size: 16\n",
      "Now the best n is [1000, 979, 936] and the number of samples is 50 and the number of epochs is 25 and the batch size is 8\n",
      "Best (n1, n2, n3): [898, 898, 23], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 96.0%\n",
      "Epochs: 25, Batch size: 8\n",
      "Best (n1, n2, n3): [979, 883, 763], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 96.2%\n",
      "Epochs: 25, Batch size: 8\n",
      "Best (n1, n2, n3): [799, 681, 459], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 96.3%\n",
      "Epochs: 25, Batch size: 8\n",
      "Best (n1, n2, n3): [778, 761, 737], Best Activations: ['relu', 'relu', 'relu', 'relu'], Accuracy: 96.4%\n",
      "Epochs: 25, Batch size: 8\n",
      "Now the best n is [778, 761, 737] and the number of samples is 25 and the number of epochs is 35 and the batch size is 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 122\u001b[0m\n\u001b[0;32m    119\u001b[0m         batch_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNow the best n is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and the number of samples is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msamples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and the number of epochs is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and the batch size is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[68], line 107\u001b[0m, in \u001b[0;36mrandom_search\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m) \u001b[38;5;66;03m# Fixed learning rate for simplicity\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m--> 107\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m test(val_dataloader, model)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "Cell \u001b[1;32mIn[68], line 56\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\mnishino-admin\\workspace\\CAP5619-sp24\\CAP5619-SP24_win\\lib\\site-packages\\torch\\_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mnishino-admin\\workspace\\CAP5619-sp24\\CAP5619-SP24_win\\lib\\site-packages\\torch\\_dynamo\\decorators.py:46\u001b[0m, in \u001b[0;36mdisable\u001b[1;34m(fn, recursive)\u001b[0m\n\u001b[0;32m     44\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[1;32m---> 46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mnishino-admin\\workspace\\CAP5619-sp24\\CAP5619-SP24_win\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:437\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 437\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\inspect.py:826\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28many\u001b[39m(filename\u001b[38;5;241m.\u001b[39mendswith(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    824\u001b[0m              importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mEXTENSION_SUFFIXES):\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[0;32m    828\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define a 4 layer ReLU neural network\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, n1, n2, n3, activations):\n",
    "        \"\"\" Define the layers of the neural network.\n",
    "        \n",
    "        Args:\n",
    "            n1: the number of neurons in the first layer.\n",
    "            n2: the number of neurons in the second layer.\n",
    "            n3: the number of neurons in the third layer.\n",
    "            activations: a list of strings, each string is the name of the activation function to use in the corresponding layer.\n",
    "            \"\"\"\n",
    "        \n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.activation_functions = {\n",
    "            \"relu\": torch.nn.ReLU(),\n",
    "            \"sigmoid\": torch.nn.Sigmoid()\n",
    "        }\n",
    "        self.layer1 = torch.nn.Linear(256, n1)\n",
    "        self.layer2 = torch.nn.Linear(n1, n2)\n",
    "        self.layer3 = torch.nn.Linear(n2, n3)\n",
    "        self.layer4 = torch.nn.Linear(n3, 10)\n",
    "\n",
    "        self.activations = torch.nn.ModuleList([self.activation_functions[activation] for activation in activations])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.activations[0](x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activations[1](x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.activations[2](x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.activations[3](x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model - This corresponds to 1 epoch\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Test the model\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    return correct\n",
    "\n",
    "def random_search_4layerReLU():\n",
    "    \"\"\" Random search for 4 layer ReLU neural network hyperparameters.\n",
    "\n",
    "    We will employ the \"coarse to fine\" strategy on the number of neurons in each layer.\n",
    "    At the beginning, each layer will have a range of 10 to 2024 neurons. \n",
    "    Then, we will reduce the range to 10 to the best number of neurons found in the previous iteration so that we can find the best number of neurons in a smaller range.\n",
    "    Moreover, we will double the number of samples, +10 the epochs, and half the batch size in each iteration to get a more accurate result.\n",
    "    \n",
    "    \"\"\"\n",
    "    best_accuracy = 0\n",
    "    best_n = [1024, 1024, 1024]\n",
    "    new_best_n = best_n\n",
    "    samples = 200\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "\n",
    "    for _ in range(3):\n",
    "        # Create dataloaders\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "        for _ in range(samples):\n",
    "            n3 = torch.randint(10, max(best_n[2],11), (1,)).item()\n",
    "            n2 = torch.randint(n3, max(best_n[1],11), (1,)).item()\n",
    "            n1 = torch.randint(n2, max(best_n[0],11), (1,)).item()\n",
    "\n",
    "            activation_options = ['relu'] \n",
    "            activations = [random.choice(activation_options) for _ in range(4)]\n",
    "\n",
    "            model = NeuralNetwork(n1, n2, n3, activations).to(device)\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=1e-2) # Fixed learning rate for simplicity\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                train(train_dataloader, model, loss_fn, optimizer)\n",
    "            \n",
    "            accuracy = test(val_dataloader, model, loss_fn)\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                new_best_n = [n1, n2, n3]\n",
    "                print(f\"Best (n1, n2, n3): {new_best_n}, Best Activations: {activations}, Accuracy: {(100*best_accuracy):>0.1f}%\")\n",
    "                print(f\"Epochs: {epochs}, Batch size: {batch_size}\")\n",
    "        best_n = new_best_n\n",
    "\n",
    "        samples //= 2\n",
    "        epochs += 10\n",
    "        batch_size //= 2\n",
    "        print(f\"Now the best n is {best_n} and the number of samples is {samples} and the number of epochs is {epochs} and the batch size is {batch_size}\")\n",
    "\n",
    "random_search_4layerReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally Connected NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocallyConnected2D(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(LocallyConnected2D, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.weights = torch.nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(out_channels))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.unfold(x, self.kernel_size, padding=self.padding, stride=self.stride)\n",
    "        x = x.permute(0, 2, 1).unsqueeze(2)\n",
    "        x = torch.matmul(x, self.weights.view(self.weights.size(0), -1).t()).squeeze(2).permute(0, 2, 1)\n",
    "        x = x + self.bias\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(64*4*4, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.conv1(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 64*4*4)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAP5619-SP24_win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
