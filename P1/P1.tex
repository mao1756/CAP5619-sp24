%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% fphw Assignment
% LaTeX Template
% Version 1.0 (27/04/2019)
%
% This template originates from:
% https://www.LaTeXTemplates.com
%
% Authors:
% Class by Felipe Portales-Oliva (f.portales.oliva@gmail.com) with template 
% content and modifications by Vel (vel@LaTeXTemplates.com)
%
% Template (this file) License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
	12pt, % Default font size, values between 10pt-12pt are allowed
	%letterpaper, % Uncomment for US letter paper size
	%spanish, % Uncomment for Spanish
]{../Template/fphw}

% Template-specific packages
\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{mathpazo} % Use the Palatino font

\usepackage{graphicx} % Required for including images

\usepackage{booktabs} % Required for better horizontal rules in tables

\usepackage{listings} % Required for insertion of code

\usepackage{enumerate} % To modify the enumerate environment

% Additional packages needed
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{mwe}
\usepackage{comment}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}

%----------------------------------------------------------------------------------------
%	ASSIGNMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Programming Project \#1} % Assignment title

\author{Mao Nishino, Farez Siddiqui} % Student name

\date{March 21st, 2024} % Due date

\institute{Florida State University \\ Department of Computer Science} % Institute or school name

\class{Deep and Reinforcement Learning Fundamentals (CAP5619-0001.sp24)} % Course or class name

\professor{Dr. Xiuwen Liu} % Professor or teacher in charge of the assignment

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Output, the assignment title, created automatically using the information in the custom commands above

%----------------------------------------------------------------------------------------
%	ASSIGNMENT CONTENT
%----------------------------------------------------------------------------------------

\section*{Task I - Neural Network Design}

\begin{problem}
In the deep learning framework you have set up, design three different neural networks. Each one must have
at least four layers; rectified activation functions must be used at least in one of the layers (among all the
three networks) and sigmoid/tanh activation functions must be used in one of the layers as well (among
all the three networks). The overall connection requirements are as follows:
\begin{enumerate}[label = (\arabic*)]
\item Fully connected, where each input/neuron is connected to all the neurons in the next layer
\item Locally connected with no weights shared in the first three layers, where each input/neuron is
connected to the neurons in a local neighbor in the next layer
\item Locally connected with weights shared in the first three layers (i.e., a convolutional neural network)
\end{enumerate}
In your report, you need to provide explanations of your design choices and describe your neural networks clearly.
\end{problem}

%------------------------------------------------

\subsection*{Answer}
The architectures for each network are shown in Figure \ref{fig:p1_architectures}. We will give detailed descriptions and explanations behind the design choices.
\begin{enumerate}[label = (\arabic*)] 
	\item This is a fully connected four-layer neural network with ReLU as activation functions.  We have chosen ReLU as the activation function because this is the standard default choice \cite[p188]{Goodfellow-et-al-2016} and other choices such as the sigmoidal function or tanh are discouraged for saturation issues to use in hidden layers\cite[p191]{Goodfellow-et-al-2016}. As for the number of layers, we have chosen 4 simply because it was the minimal number we were required to have.  
 
    \textbf{Extra Credit Attempt: Architecture Optimization} We have employed the commonly used "coarse-to-fine" strategy to choose the number of neurons in each layer. The detailed algorithm is stated in Algorithm \ref{alg:NAS}. We have predetermined any hyperparameters other than the number of neurons for simplicity and ease of training. Still, the idea of the algorithm is clearly applicable in more general search spaces. The basic idea of the algorithm is to conduct random searches in increasingly smaller search spaces. Although there are other search strategies, such as reinforcement learning-based or evolutionary algorithm-based algorithms \cite[p68]{automl}, we have chosen a random search-based algorithm because it is simple to implement. Although random search performs worse than the aforementioned methods, the gain from choosing them is often small \cite[p69]{automl}. To alleviate the computational cost, we start from the models trained with less training time. We gradually increase the training time by increasing the number of epochs and decreasing the batch size. That is, we take a common strategy of evaluating performance in less complicated situations and gradually increasing complexity for more accurate performance estimation, as recommended in \cite[p71]{automl}.
    
	\item Since this is the convolutional network without weight sharing, we have replaced the convolution layers in the convolutional network we introduce later with locally connected layers. As \verb|PyTorch|\cite{pytorch} does not have locally connected layers implemented, we have implemented ourselves, and the implementation details can be found in our notebook. Moreover, to satisfy the requirement to have a sigmoidal activation function somewhere, we used it for our output layer. As mentioned, we avoided using it in hidden layers due to the saturation issue.

    \item As our starting point, we have looked at one of the earliest papers on the convolutional network \cite{convnet_original}, which also works on the same data as ours. Their architecture already boasts a test error rate of 3.4\%, but the choice of layers is outdated in the current standard. So, we have taken their architecture and modernized the design, following the recommendations in \cite{Goodfellow-et-al-2016}. We note that \cite{convnet_original} puts no activation functions between convolutional layers and pooling layers, which is against the common architecture noted in \cite[p336]{Goodfellow-et-al-2016}, so we introduced ReLU functions there. Moreover, although they use average pooling, we switched to maximum pooling, which is more commonly used.
    
\end{enumerate}


\begin{figure}[!htbp]
    \centering

    





\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,461); %set diagram left start at 0, and has height of 461

%Flowchart: Process [id:dp02351632173585183] 
\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (112.25,11.5) -- (221.75,11.5) -- (221.75,32) -- (112.25,32) -- cycle ;
%Flowchart: Process [id:dp42941465319220407] 
\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (112.25,41.5) -- (221.75,41.5) -- (221.75,62) -- (112.25,62) -- cycle ;
%Flowchart: Process [id:dp08453394336733022] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (112.25,72.5) -- (221.75,72.5) -- (221.75,93) -- (112.25,93) -- cycle ;
%Flowchart: Process [id:dp4778014521769103] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (113.25,103) -- (222.75,103) -- (222.75,123.5) -- (113.25,123.5) -- cycle ;
%Flowchart: Process [id:dp9306397015209844] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (112.75,132) -- (222.25,132) -- (222.25,152.5) -- (112.75,152.5) -- cycle ;
%Flowchart: Process [id:dp9050309774198928] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (112.75,162.25) -- (222.25,162.25) -- (222.25,182.75) -- (112.75,182.75) -- cycle ;
%Flowchart: Process [id:dp6677905436834566] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (112.5,191.75) -- (222,191.75) -- (222,212.25) -- (112.5,212.25) -- cycle ;
%Flowchart: Process [id:dp05592789910796103] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (112.25,222.25) -- (221.75,222.25) -- (221.75,242.75) -- (112.25,242.75) -- cycle ;
%Flowchart: Process [id:dp9952366943934081] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (112.25,252.25) -- (221.75,252.25) -- (221.75,272.75) -- (112.25,272.75) -- cycle ;
%Flowchart: Process [id:dp8718199900480916] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (112.4,282.25) -- (221.9,282.25) -- (221.9,302.75) -- (112.4,302.75) -- cycle ;
%Straight Lines [id:da9533929994486752] 
\draw    (166.75,32) -- (167,38.25) ;
\draw [shift={(167.13,41.25)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da28583644695644894] 
\draw    (167.5,63.25) -- (167.75,69.5) ;
\draw [shift={(167.88,72.5)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da03204029710574252] 
\draw    (167.95,92.8) -- (168.2,99.05) ;
\draw [shift={(168.33,102.05)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da7872545420609312] 
\draw    (168.7,124.05) -- (168.95,130.3) ;
\draw [shift={(169.08,133.3)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da32793245802759863] 
\draw    (168.88,303) -- (169.14,309.25) ;
\draw [shift={(169.26,312.25)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da8267605040931463] 
\draw    (168.97,273.25) -- (169.22,279.5) ;
\draw [shift={(169.34,282.5)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da9742587362750603] 
\draw    (168.08,152.7) -- (168.34,158.95) ;
\draw [shift={(168.46,161.95)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da008937420928793793] 
\draw    (168.83,182.95) -- (169.09,189.2) ;
\draw [shift={(169.21,192.2)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da10562370725328729] 
\draw    (169.62,212.83) -- (169.87,219.09) ;
\draw [shift={(169.99,222.08)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da5036344051727422] 
\draw    (170.03,243.08) -- (170.29,249.34) ;
\draw [shift={(170.41,252.33)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Flowchart: Process [id:dp07654239667008067] 
\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (271.75,11.17) -- (381.25,11.17) -- (381.25,31.67) -- (271.75,31.67) -- cycle ;
%Flowchart: Process [id:dp37376964215801745] 
\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (271.75,41.17) -- (381.25,41.17) -- (381.25,61.67) -- (271.75,61.67) -- cycle ;
%Flowchart: Process [id:dp3428413998103992] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (271.75,72.17) -- (381.25,72.17) -- (381.25,92.67) -- (271.75,92.67) -- cycle ;
%Flowchart: Process [id:dp629266102559507] 
\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (272.75,102.67) -- (382.25,102.67) -- (382.25,123.17) -- (272.75,123.17) -- cycle ;
%Flowchart: Process [id:dp7074374276144555] 
\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (272.25,131.67) -- (381.75,131.67) -- (381.75,152.17) -- (272.25,152.17) -- cycle ;
%Flowchart: Process [id:dp15281786912615014] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (272.25,161.92) -- (381.75,161.92) -- (381.75,182.42) -- (272.25,182.42) -- cycle ;
%Flowchart: Process [id:dp556820525114857] 
\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (272,191.42) -- (381.5,191.42) -- (381.5,211.92) -- (272,211.92) -- cycle ;
%Flowchart: Process [id:dp6854555639120627] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (272.5,251.67) -- (382,251.67) -- (382,272.17) -- (272.5,272.17) -- cycle ;
%Flowchart: Process [id:dp34589582378519923] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (272.25,282.42) -- (381.75,282.42) -- (381.75,302.92) -- (272.25,302.92) -- cycle ;
%Flowchart: Process [id:dp4199329151131661] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (272.4,312.42) -- (381.9,312.42) -- (381.9,332.92) -- (272.4,332.92) -- cycle ;
%Straight Lines [id:da29287117438362475] 
\draw    (326.25,31.67) -- (326.5,37.92) ;
\draw [shift={(326.63,40.92)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da8715728513895342] 
\draw    (327,62.92) -- (327.25,69.17) ;
\draw [shift={(327.38,72.17)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da2767853528454396] 
\draw    (327.45,92.47) -- (327.7,98.72) ;
\draw [shift={(327.83,101.72)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da9139221237028485] 
\draw    (328.2,123.72) -- (328.45,129.97) ;
\draw [shift={(328.58,132.97)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da334115025888331] 
\draw    (328.88,333.17) -- (329.14,339.42) ;
\draw [shift={(329.26,342.42)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da623702185150133] 
\draw    (328.97,303.42) -- (329.22,309.67) ;
\draw [shift={(329.34,312.67)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da12298939627023597] 
\draw    (327.58,152.37) -- (327.84,158.62) ;
\draw [shift={(327.96,161.62)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da8763601037493007] 
\draw    (328.33,182.62) -- (328.59,188.87) ;
\draw [shift={(328.71,191.87)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da6421658225619669] 
\draw    (329.12,212.5) -- (329.37,218.75) ;
\draw [shift={(329.49,221.75)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da35909666112314986] 
\draw    (328.03,272.75) -- (328.29,279) ;
\draw [shift={(328.41,282)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Flowchart: Process [id:dp49794632990715604] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (272.25,342.42) -- (381.75,342.42) -- (381.75,362.92) -- (272.25,362.92) -- cycle ;
%Straight Lines [id:da26795199376819423] 
\draw    (328.88,364.17) -- (329.14,370.42) ;
\draw [shift={(329.26,373.42)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Flowchart: Process [id:dp5682047520026112] 
\draw  [fill={rgb, 255:red, 255; green, 255; blue, 255 }  ,fill opacity=1 ] (432.25,11.5) -- (541.75,11.5) -- (541.75,32) -- (432.25,32) -- cycle ;
%Flowchart: Process [id:dp6597728419761311] 
\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (432.25,41.5) -- (541.75,41.5) -- (541.75,62) -- (432.25,62) -- cycle ;
%Flowchart: Process [id:dp8809875064098089] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (432.25,72.5) -- (541.75,72.5) -- (541.75,93) -- (432.25,93) -- cycle ;
%Flowchart: Process [id:dp985728348238234] 
\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (433.25,103) -- (542.75,103) -- (542.75,123.5) -- (433.25,123.5) -- cycle ;
%Flowchart: Process [id:dp7774865454293953] 
\draw  [fill={rgb, 255:red, 245; green, 166; blue, 35 }  ,fill opacity=1 ] (432.75,132) -- (542.25,132) -- (542.25,152.5) -- (432.75,152.5) -- cycle ;
%Flowchart: Process [id:dp70641917713161] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (432.75,162.25) -- (542.25,162.25) -- (542.25,182.75) -- (432.75,182.75) -- cycle ;
%Flowchart: Process [id:dp12395780365696174] 
\draw  [fill={rgb, 255:red, 74; green, 144; blue, 226 }  ,fill opacity=1 ] (432.5,191.75) -- (542,191.75) -- (542,212.25) -- (432.5,212.25) -- cycle ;
%Flowchart: Process [id:dp5481496040002598] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (433,252.5) -- (542.5,252.5) -- (542.5,273) -- (433,273) -- cycle ;
%Flowchart: Process [id:dp2536457567596164] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (432.75,282.75) -- (542.25,282.75) -- (542.25,303.25) -- (432.75,303.25) -- cycle ;
%Flowchart: Process [id:dp4504416882648943] 
\draw  [fill={rgb, 255:red, 208; green, 2; blue, 27 }  ,fill opacity=1 ] (432.9,312.75) -- (542.4,312.75) -- (542.4,333.25) -- (432.9,333.25) -- cycle ;
%Straight Lines [id:da7316233557457539] 
\draw    (486.75,32) -- (487,38.25) ;
\draw [shift={(487.13,41.25)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da5787630510498825] 
\draw    (487.5,63.25) -- (487.75,69.5) ;
\draw [shift={(487.88,72.5)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da9496017529080909] 
\draw    (487.95,92.8) -- (488.2,99.05) ;
\draw [shift={(488.33,102.05)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da5761985726153773] 
\draw    (488.7,124.05) -- (488.95,130.3) ;
\draw [shift={(489.08,133.3)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da7556895771636609] 
\draw    (489.38,333.5) -- (489.64,339.75) ;
\draw [shift={(489.76,342.75)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da2705787530611572] 
\draw    (489.47,303.75) -- (489.72,310) ;
\draw [shift={(489.84,313)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da6880617384248] 
\draw    (488.08,152.7) -- (488.34,158.95) ;
\draw [shift={(488.46,161.95)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da5921195014182679] 
\draw    (488.83,182.95) -- (489.09,189.2) ;
\draw [shift={(489.21,192.2)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da31472140137421234] 
\draw    (489.62,212.83) -- (489.87,219.09) ;
\draw [shift={(489.99,222.08)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da23412453984797033] 
\draw    (490.53,273.58) -- (490.79,279.84) ;
\draw [shift={(490.91,282.83)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Flowchart: Process [id:dp32985082224910545] 
\draw  [fill={rgb, 255:red, 80; green, 227; blue, 194 }  ,fill opacity=1 ] (432.75,342.75) -- (542.25,342.75) -- (542.25,363.25) -- (432.75,363.25) -- cycle ;
%Straight Lines [id:da09961157577092683] 
\draw    (489.38,364.5) -- (489.64,370.75) ;
\draw [shift={(489.76,373.75)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Flowchart: Process [id:dp2801122247121537] 
\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (272.5,222.25) -- (382,222.25) -- (382,242.75) -- (272.5,242.75) -- cycle ;
%Flowchart: Process [id:dp6111010427912171] 
\draw  [fill={rgb, 255:red, 126; green, 211; blue, 33 }  ,fill opacity=1 ] (432.75,222.75) -- (542.25,222.75) -- (542.25,243.25) -- (432.75,243.25) -- cycle ;
%Straight Lines [id:da1785729824658211] 
\draw    (328.03,242.75) -- (328.29,249) ;
\draw [shift={(328.41,252)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;
%Straight Lines [id:da5970664580138136] 
\draw    (490.03,243.75) -- (490.29,250) ;
\draw [shift={(490.41,253)}, rotate = 267.68] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.08]  [draw opacity=0] (6.25,-3) -- (0,0) -- (6.25,3) -- cycle    ;

% Text Node
\draw (125.75,13) node [anchor=north west][inner sep=0.75pt]   [align=left] {Input (16,16)};
% Text Node
\draw (141,43.25) node [anchor=north west][inner sep=0.75pt]   [align=left] {Flatten()};
% Text Node
\draw (119.08,76.17) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Linear(256,778)};
% Text Node
\draw (143.25,104.75) node [anchor=north west][inner sep=0.75pt]   [align=left] {ReLU()};
% Text Node
\draw (119.33,135.92) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Linear(778,761)};
% Text Node
\draw (142.75,164.25) node [anchor=north west][inner sep=0.75pt]   [align=left] {ReLU()};
% Text Node
\draw (118.58,195.67) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Linear(761,737)};
% Text Node
\draw (143,223.75) node [anchor=north west][inner sep=0.75pt]   [align=left] {ReLU()};
% Text Node
\draw (124.6,256) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Linear(737,10)};
% Text Node
\draw (142.9,284.25) node [anchor=north west][inner sep=0.75pt]   [align=left] {ReLU()};
% Text Node
\draw (285.25,12.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {Input (16,16)};
% Text Node
\draw (260,45.67) node [anchor=north west][inner sep=0.75pt]  [font=\scriptsize] [align=left] {\begin{minipage}[lt]{100.82pt}\setlength\topsep{0pt}
\begin{center}
LocallyCon. 2D(4,5)
\end{center}

\end{minipage}};
% Text Node
\draw (301.25,73.67) node [anchor=north west][inner sep=0.75pt]   [align=left] {ReLU()};
% Text Node
\draw (276,105.67) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Maxpool2d(2,1)};
% Text Node
\draw (274.5,135.92) node [anchor=north west][inner sep=0.75pt]   [align=left] {{\scriptsize LocallyCon. 2D (12,3)}};
% Text Node
\draw (302.25,163.92) node [anchor=north west][inner sep=0.75pt]   [align=left] {ReLU()};
% Text Node
\draw (276,194.67) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Maxpool2d(2,2)};
% Text Node
\draw (285,255.67) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Linear(192,84)};
% Text Node
\draw (305.1,284.17) node [anchor=north west][inner sep=0.75pt]   [align=left] {ReLU()};
% Text Node
\draw (287.4,315.92) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Linear(84,10)};
% Text Node
\draw (245,386.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {\begin{minipage}[lt]{120pt}\setlength\topsep{0pt}
\begin{center}
Output: \\Unnormalized Logit
\end{center}

\end{minipage}};
% Text Node
\draw (296.25,344.42) node [anchor=north west][inner sep=0.75pt]   [align=left] {Sigmoid()};
% Text Node
\draw (445.75,13) node [anchor=north west][inner sep=0.75pt]   [align=left] {Input (16,16)};
% Text Node
\draw (442.75,34.5) node [anchor=north west][inner sep=0.75pt]  [font=\normalsize] [align=left] {\begin{minipage}[lt]{61.69pt}\setlength\topsep{0pt}
\begin{center}
Conv2d(4,5) 
\end{center}

\end{minipage}};
% Text Node
\draw (461.75,74) node [anchor=north west][inner sep=0.75pt]   [align=left] {ReLU()};
% Text Node
\draw (435,106) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Maxpool2d(2,1)};
% Text Node
\draw (462.75,164.25) node [anchor=north west][inner sep=0.75pt]   [align=left] {ReLU()};
% Text Node
\draw (435,195) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Maxpool2d(2,2)};
% Text Node
\draw (445,255) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Linear(192,84)};
% Text Node
\draw (465.1,283.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {ReLU()};
% Text Node
\draw (447.9,316.25) node [anchor=north west][inner sep=0.75pt]  [font=\small] [align=left] {Linear(84,10)};
% Text Node
\draw (464.75,343.25) node [anchor=north west][inner sep=0.75pt]   [align=left] {ReLU()};
% Text Node
\draw (438.75,125) node [anchor=north west][inner sep=0.75pt]  [font=\normalsize] [align=left] {\begin{minipage}[lt]{67.36pt}\setlength\topsep{0pt}
\begin{center}
Conv2d(12,3) 
\end{center}

\end{minipage}};
% Text Node
\draw (92.25,243.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Fully Connected Network};
% Text Node
\draw (251.75,244.5) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Locally Connected Network};
% Text Node
\draw (413.75,226) node [anchor=north west][inner sep=0.75pt]  [rotate=-270] [align=left] {Convolutional Network};
% Text Node
\draw (300.75,223) node [anchor=north west][inner sep=0.75pt]   [align=left] {Flatten()};
% Text Node
\draw (461.5,224.5) node [anchor=north west][inner sep=0.75pt]   [align=left] {Flatten()};


\end{tikzpicture}
    \caption{Neural network architectures. We use the name of layers from \texttt{PyTorch}\cite{pytorch}. For \texttt{Linear} layers, the parameters are in the format (\texttt{in\_features}, \texttt{out\_features}). For \texttt{LocallyConnected2D} (our implementation of a locally connected layer) and \texttt{Conv2d} layers, they represent (\texttt{out\_features}, \texttt{kernel\_size}). For \texttt{Maxpool2d}, the parameters are (\texttt{kernel\_size}, \texttt{stride}). Parameters not specified are set to their default values.}
    \label{fig:p1_architectures}

\end{figure}

\begin{algorithm}
        \caption{The Coarse-to-Fine Random Search Algorithm}\label{alg:NAS}
        \begin{algorithmic}
         \Require An initial number of neurons $(n_1,n_2,n_3)$
         \Require An initial number of random samples $s$
         \Require An initial number of epochs $e$
         \Require An initial batch size $b$
         \Require The number of rounds $r$
         \State Initialize the best $n$ in a round: $(N_1,N_2,N_3) = (n_1,n_2,n_3)$
        \State Initialize the temporary storage for $N_1,N_2,N_3$. We only update them after the end of a round to avoid shrinking the search space too fast.$(\bar{N}_1,\bar{N}_2,\bar{N}_3) = (N_1,N_2,N_3)$
         \State Initialize the best accuracy: $a=0$
         \For{rounds=$1,\cdots , r$}
            \For{sample=$1,\cdots ,s$}
                \State Pick $\hat{n_1},\hat{n_2}$ and $\hat{n_3}$ in descending order. We bound them by $(N_1,N_2,N_3)$ so that the search space gets smaller as we go:
                \State Pick $\hat{n_3}$ randomly from $10,11,\cdots \max(N_3, 11)$
                \State Pick $\hat{n_2}$ randomly from $\hat{n_3},\cdots \textrm{max}(N_2,11)$
                \State Pick $\hat{n_1}$ randomly from $\hat{n}_2,\cdots \max(N_1,11)$
                \State Train a neural network where $\hat{n}_i$ is the number of layers in the $i$th layer (The number in the last layer is fixed to be $10$) for $e$ epochs with $b$ batch sizes.
                \If{Accuracy for this sample > $a$}
                \State $(\bar{N}_1,\bar{N}_2,\bar{N}_3) \leftarrow (\hat{n}_1,\hat{n}_2,\hat{n}_3)$
                \State $a \leftarrow$ Accuracy for this sample
                \EndIf
            \EndFor
            \State $(N_1,N_2,N_3) \leftarrow (\hat{N}_1,\hat{N}_2,\hat{N}_3)$
            \State Decrease the number of samples for computational expense and increase/decrease the number of epochs and batch size to increase the complexity of the model.
            \State $s\leftarrow s/2$
            \State $e \leftarrow e+10$
            \State $b \leftarrow b/2$
         \EndFor
        \end{algorithmic}
\end{algorithm}
%----------------------------------------------------------------------------------------

\section*{Task II - Techniques for Optimization}

\begin{problem}
You need to do the required analysis and then perform experiments.
\begin{enumerate}[label = (\arabic*)]
\item Parameter initialization strategies. For each of the three networks, analyze how the parameters should
be initialized. Then demonstrate three cases based on your analysis: 1) learning is very slow; 2)
learning is effective (i.e., fast with accurate results); and 3) the learning is too fast (i.e., the network
does not give good performance).
\item Learning rate. Estimate a good learning rate for each of the three networks. Then demonstrate three
cases based on your analysis: 1) learning is very slow; 2) learning is effective; and 3) learning is too
fast.
\item Explain how the batch size would impact the batch normalization for your convolutional neural
network. Then demonstrate an effective batch size and an ineffective batch size on the convolutional
neural network you have.
\item Momentum. Commonly used momentum coefficient values are 0.5, 0.9, and 0.99. Using the best
parameter initialization strategy, the best learning rate, and the best batch size you have found so far,
experiment with the three different momentum values on the convolutional neural network you have
and document the results. Explain the differences you have observed on the convolutional neural
network you have.
\end{enumerate}

\end{problem}

%------------------------------------------------

\subsection*{Answer}

\begin{enumerate}[label = (\arabic*)] % Sub-questions styled as italic letters
	\item In general, we need to take care of two things in initialization: (i) Breaking the symmetry and (ii) uniform learning. Breaking symmetry refers to the strategy where we avoid initializing multiple weights by the same value. If we initialize by the same weight, using the same input and a deterministic learning algorithm would yield the same output \cite[p298]{Goodfellow-et-al-2016}, which is not efficient.  Uniform learning refers to the idea that all weights should be learned at the same rate, which again is concerned with the efficiency of the training. Moreover, when we have the ReLU activation, we need to initialize the biases by small positive numbers to avoid the dying neurons problem. Keeping this in mind, we test the following strategies adapted from \cite{Hosny2017} for all three networks. We only apply these strategies to weights and set all biases to $0.1$. This follows the recommendation in \cite[p298]{Goodfellow-et-al-2016} that we should set weights randomly, and biases should be set as a heuristically chosen constant.
    \begin{enumerate}
        \item \textbf{Zero} and \textbf{One}. We set all of the weights to be either zeros or ones.
        
        \item \textbf{Normal}. We choose the weights from the normal distribution with mean $0$ and standard deviation $0.1$. We have also tested the standard normal distribution ($\textrm{SD}=1$), but we noticed an overflow in the initial loss (hence, the loss for all epochs was \verb|nan|).
        \item \textbf{Uniform}. We choose the weights from the uniform distribution on $[0,1]$.
        \item \textbf{Truncated Normal}. We choose the weights from the standard truncated normal distribution on $[-0.1,0.1]$.
        \item \textbf{Uniform Xavier (Glorot).} We choose the weights from the uniform distribution on $(-a,a)$ where $a = \sqrt{\frac{6}{n_{in}+n_{out}}}$, and $n_{in}$ and $n_{out}$ is determined as follows: For a fully connected layer, $n_{in}$ is the number of inputs and $n_{out}$ is the number of outputs. For a convolutional layer, $n_{in}$ is equal to $(\textrm{the number of input features})\times (\textrm{kernel size})^2$, and $n_{out}$ is similar with output features. We use the same $n_{in}$ and $n_{out}$ for a locally connected layer.
    \item \textbf{Normal Xavier (Glorot).} We choose the weights from the uniform distribution of mean $0$ and the variance $\frac{2}{n_{in}+n_{out}}$ where $n_{in}$ and $n_{out}$. These parameters have the same meaning as previously.
    \item \textbf{Orthogonal.} We initialize the weights by an orthogonal matrix. If the weights tensor have the dimension higher than $2$, we flatten the extraneous dimensions to get a $2$-D tensor (a matrix).
    
    The result for the fully connected network is shown in Figure \ref{fig:init_fc}.
    \end{enumerate}

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=\textwidth]{initialization_comparison}
        \caption{Comparison of initialization methods for the fully connected network. The legend is shared in both figures.}
        \label{fig:init_fc}
    \end{figure}
    From this figure, we can classify the initialization method in three cases:
    \begin{enumerate}[label=(\roman*)]
        \item We believe that no method learns too slowly. In fact, we can observe that all initialization methods led to convergence to some local minima fairly quickly, although some converged to bad local minima.
        \item The learning is effective for \textbf{Normal Xavier} and \textbf{Truncated Normal}.
        \item The learning is too fast for  \textbf{Zero}, \textbf{One}, \textbf{Uniform} \textbf{Normal}, \textbf{Orthogonal}, \textbf{Uniform Xavier}. They do not generalize well compared to Normal Xavier and Truncated Normal.
    \end{enumerate}
    We will now consider the locally connected network. Figure  \ref{fig:init_lc} shows the result for the locally connected network.

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=\textwidth]{initialization_comparison_lc}
        \caption{Comparison of initialization methods for the locally connected network.}
        \label{fig:init_lc}
    \end{figure}

    From this figure, we can classify the initialization method in three cases:
    \begin{enumerate}[label=(\roman*)]
        \item The learning is too slow for  \textbf{Zero}, \textbf{Orthogonal} and \textbf{Truncated Normal}. In a separate experiment, we observed that Orthogonal and Truncated Normal can be as accurate as effective methods with more epochs.
        \item The learning is effective for \textbf{Normal Xavier}, \textbf{Uniform Xavier} and \textbf{Normal}.
        \item The training is too fast for \textbf{One}, and \textbf{Uniform} and they converged to some bad local minima.
    \end{enumerate}

    Finally, we consider the convolutional network. Figure \ref{fig:init_conv} shows the result for the convolutional network.

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=\textwidth]{initialization_comparison_conv}
        \caption{Comparison of initialization methods for the convolutional network.}
        \label{fig:init_conv}
    \end{figure}

    From this figure, we can classify the initialization method in three cases:
    \begin{enumerate}[label=(\roman*)]
        \item We believe there is no method that is too slow. All methods led to convergence fairly quickly.
        \item The learning is effective for \textbf{Uniform Xavier}.
        \item The training is too fast for any other methods. Although some methods are distinctively better, they converged to some bad local minima and show bad generalization compared to Uniform Xavier.
    \end{enumerate}
    
	\item Figure \ref{fig:fc_lr} shows the comparison between different learning rates for the fully connected network. We fixed the seeds of the random number generator to obtain the same initialization. The losses at the beginning are different because these are losses after one update.
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=\textwidth]{lr_comparison_fc}
        \caption{Comparison of learning rates for the fully connected network.}
        \label{fig:fc_lr}
    \end{figure}
    
    Based on this figure, we can classify the learning rates in three cases:
    \begin{enumerate}[label = (\roman*)]
        \item Learning is too slow for 1e-5, 0.001, 0.01 and 0.1.
        \item Learning is effective for 0.2 and 0.0001.
        \item Learning is too fast for 0.5.
    \end{enumerate}
    We will now consider the locally connected network. Figure \ref{fig:lc_lr} shows the result for the locally connected network.
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=\textwidth]{lr_comparison_lc}
        \caption{Comparison of learning rates for the locally connected network.}
        \label{fig:lc_lr}
    \end{figure}

    Based on this figure, we can classify the learning rates in three cases:
    \begin{enumerate}[label = (\roman*)]
        \item Learning is too slow for 1e-5, 0.0001, 0.001, 0.01.
        \item Learning is effective for 0.1, 0.2, 0.5.
        \item We believe that no learning rate learns too fast.
    \end{enumerate}

    Finally, we consider the convolutional network. Figure \ref{fig:conv_lr} shows the result.
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=\textwidth]{lr_comparison_conv}
        \caption{Comparison of learning rates for the convolutional network.}
        \label{fig:conv_lr}
    \end{figure}
    Based on this figure, we can classify the learning rates in three cases:
    \begin{enumerate}[label = (\roman*)]
        \item Learning is too slow for 1e-5, 0.0001, 0.001, 0.01.
        \item Learning is effective for 0.1, 0.2.
        \item Learning is too fast for 0.5.
    \end{enumerate}
    \newpage\item Batch normalization is an adaptive reparametrization method meant to deal with the problem of covariate shift found in deep networks. Covariate shift arises when the distribution of the test dataset used in training differs significantly enough from the test data so as to cause poor generalization performance. This happens in deep networks because changes in the distribution of the test dataset cause changes in the distribution of network activations, and since deep networks are composition-upon-composition of network activations, this shift can propagate throughout the network and cause both slower learning and poor generalization performance.\\\\
    Batch normalization seeks to address this taking advantage of the fact that network training converges faster with "whitened" inputs to each layer (decorrelate and linearly transform the inputs to have to have mean 0 and variance 1.) This is done by subtracting the empirical mean from the output layer and dividing by the empirical standard deviation. These statistics are computed from the batch used in the optimization method, and therein lies where batch size will have an impact on this process. From this setup, it's clear that larger batch sizes would act to help the effectiveness of batch normalization because the statistics of a larger batch would better represent the statistics of the entire dataset which would lead to more accurate normalization. On the other hand, it is known that smaller batch sizes lead to better generalization performance, so there is an aspect of picking too large of a batch size when it comes to batch normalization. Ultimately, batch size should be played around with to find an optimal size for the problem at hand.\\\\
    For the project, we implement different batch sizes to our convolutional neural network to find effective batch sizes would be for this problem. The problem did not state that we were to implement batch normalization, only to study the effect of different batch sizes, so no BatchNorm layers were added to the network to change its architecture. Batch size was adjusted in the DataLoader parameter to test 10 different batch sizes ranging from 10 to the entire training dataset. The optimal parameter initialization and learning rate we found for our CNN in the previous sections were used. The results can be found in Figure 8. From our experiments (each lasting 100 epochs), it appears that the batch sizes 16, 32, 64, 256, and 512 were effective batch sizes resulting in acceptable generalization error in 100 epochs, where as the larger batch sizes and the smaller batch size of 128 showed generally poorer performance. It could be the case that some of these sizes could have shown more promise if we had trained over more epochs, which would be a viable option seeing as each epoch took less time than their smaller batch size counterparts to train. One peculiarity is that the batch size of 128 performed so much poorer than batch sizes both smaller and larger than itself. We don't know why this anomaly may have occurred.
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=\textwidth]{batch_size_comparison}
        \caption{The comparison between different batch sizes.}
        \label{fig:batch_size_comparison}
    \end{figure}

    \item Momentum helps optimizers overcome issues with the loss surface caused by poorly conditioned Hessian matrices, as well as the inherent variance in Stochastic Gradient Descent. When the loss surface has plateaus, is very noisy, or is poorly conditioned, the most recent gradient update may push the parameters that is not optimal and slow learning down. By keeping a running tally of previous gradient updates and applying them to the most recent gradient update via an exponentially decaying sum, the gradient gains a sort of "momentum" that can help it out of problem areas of the loss surface.\\\\
    We tested 3 different values for momentum (0.5, 0.9, and 0.99) to see their effects on the speed of the learning in our convolutional neural network. The results can be found in Figure 9. Our results show that large momentum values significantly impair the network's performance. There may be a multitude of reasons that larger momentum values hurt the networks performance. It could be that the accumulated gradient is so large that the optimizer consistently overshoots the loss surface's minima, or that the higher dependence on past gradients makes the network less sensitive to new training data, effectively shutting down performance. From our results we find that using no momentum or a momentum values of 0.5 produced acceptable results.
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=\textwidth]{momentum_comparison}
        \caption{The comparison between different momentum values.}
        \label{fig:momentum_comparison}
    \end{figure}
\end{enumerate}



%----------------------------------------------------------------------------------------

\section*{Task III - Techniques for Improving Generalization}
\begin{problem}
For this task, you need to do the required analysis and apply the following regularization techniques with the
goal to improve the performance on the 2007 samples in zip\_test.txt.
\begin{enumerate}[label = (\arabic*)]
\item Use an ensemble to improve the generalization performance. Here you need to use at least three neural
networks to improve the performance of the individual neural networks. You need to analyze your results. You can use the neural networks from Task II.
\item Dropout. Explain the effects of the dropout parameter (probability of keeping a neuron) on the fully
connected neural network you have. Then demonstrate an effective case and an ineffective case on
your fully connected neural network.
\end{enumerate}
\end{problem}

%------------------------------------------------

\subsection*{Answer} 

\begin{enumerate}[label = (\arabic*)] 
	\item Ensemble methods seek to use the learned features of several models and combine their inferences to produce a better prediction. Technically speaking, this can be accomplished by training each model and at the end of each epoch, take the average of their outputs. The resulting output is the output of the ensemble model. In the case of our problem, each model's output is a 1-dimensional tensor of length 10, which represents the probability that the input is one of ten digits. We use PyTorch's \verb|torch.stack()| method to aggregate the output tensors of each model and then do an an element-wise average to get the ensemble model's output. The results can be found in Figure 10. The ensemble model does very little to improve over the convolutional neural network. We would expect this, because convolutional neural networks are significantly more suited for vision tasks such as recognizing digits than locally connected or feed forward networks. We would expect taking any insights from less well-suited networks would only prove to hurt generalization performance rather than improve it. We may have seen better results if we had used 3 convolutional neural networks with different architectures, however as it is shown the CNN we had used thus far had such good generalization performance that any improvement we could possibly glean from an ensemble model would prove marginal at best.
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=\textwidth]{ensemble_comparison}
        \caption{Comparing the generalization error of the 3 individual models and their ensemble.}
        \label{fig:ensemble_comparison}
    \end{figure}
    
	\item If the keeping probability is high, then the model observes less noise, and the model capability is high. This means that the model might overfit. If the keeping probability is low, then the model observes more noise, and the model capability is low. This means that lower probability has a more regularizing effect, but our model might underfit. Figure \ref{fig:dropout_comparison} shows the result for different dropout probabilities for our fully connected network. We inserted a dropout layer between the third layer and the fourth layer. \textbf{Note that the} \verb|prob| \textbf{parameter here is the probability of dropping out a neuron instead of keeping it.} We observe that \verb|prob|=0.1 through 0.9 works effectively except for $0.3$. We note that \verb|prob|=0 performs the worst, which is expected as there is no regularization effect.
    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=\textwidth]{dropout_comparison_fc}
        \caption{The comparison between different dropout parameters.}
        \label{fig:dropout_comparison}
    \end{figure}
\end{enumerate}
%----------------------------------------------------------------------------------------
\begin{comment}

\section*{Question 4 (bonus marks)}

\begin{problem}
	The table below shows the nutritional consistencies of two sausage types. Explain their relative differences given what you know about daily adult nutritional recommendations.
	
	\bigskip
    
	\begin{center}
		\begin{tabular}{l l l}
			\toprule
			\textit{Per 50g} & Pork & Soy \\
			\midrule
			Energy & 760kJ & 538kJ\\
			Protein & 7.0g & 9.3g\\
			Carbohydrate & 0.0g & 4.9g\\
			Fat & 16.8g & 9.1g\\
			Sodium & 0.4g & 0.4g\\
			Fibre & 0.0g & 1.4g\\
			\bottomrule
		\end{tabular}
	\end{center}
	
	\medskip
\end{problem}

%------------------------------------------------

\subsection*{Answer}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent porttitor arcu luctus, imperdiet urna iaculis, mattis eros. Pellentesque iaculis odio vel nisl ullamcorper, nec faucibus ipsum molestie. Sed dictum nisl non aliquet porttitor. Etiam vulputate arcu dignissim, finibus sem et, viverra nisl. Aenean luctus congue massa, ut laoreet metus ornare in. Nunc fermentum nisi imperdiet lectus tincidunt vestibulum at ac elit. Nulla mattis nisl eu malesuada suscipit.

%----------------------------------------------------------------------------------------

\section*{Question 5 (bonus marks)}

\begin{problem}
	\lstinputlisting[
		caption=Luftballons Perl Script, % Caption above the listing
		label=lst:luftballons, % Label for referencing this listing
		language=Perl, % Use Perl functions/syntax highlighting
		frame=single, % Frame around the code listing
		showstringspaces=false, % Don't put marks in string spaces
		numbers=left, % Line numbers on left
		numberstyle=\tiny, % Line numbers styling
	]{}
	
	\begin{enumerate}
		\item How many luftballons will be output by the Listing ???????????????\ref{lst:luftballons} above?
		\item Identify the regular expression in Listing ???????????????\ref{lst:luftballons} and explain how it relates to the anti-war sentiments found in the rest of the script.
	\end{enumerate}

\end{problem}

%------------------------------------------------

\subsection*{Answer}

\begin{enumerate}
	\item 99 luftballons.
	\item Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent porttitor arcu luctus, imperdiet urna iaculis, mattis eros. Pellentesque iaculis odio vel nisl ullamcorper, nec faucibus ipsum molestie. Sed dictum nisl non aliquet porttitor. Etiam vulputate arcu dignissim, finibus sem et, viverra nisl. Aenean luctus congue massa, ut laoreet metus ornare in. Nunc fermentum nisi imperdiet lectus tincidunt vestibulum at ac elit. Nulla mattis nisl eu malesuada suscipit.
\end{enumerate}

\end{comment}

%----------------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{P1/P1}
\end{document}
